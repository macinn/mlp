{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import inspect\n",
    "import math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Cell 0\n",
    "# Experiment: build MLP from repo (if available), otherwise fallback to local PyTorch MLP.\n",
    "# Train on a toy dataset with different activation functions and plot loss/accuracy + decision boundaries.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_N_SAMPLES = 1000\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "HIDDEN_SIZES = [64, 32]\n",
    "LR = 1e-3\n",
    "\n",
    "# Candidate module/class names to try from repo\n",
    "CANDIDATES = [\n",
    "    (\"mlp\", \"MLP\"),\n",
    "    (\"model\", \"MLP\"),\n",
    "    (\"models.mlp\", \"MLP\"),\n",
    "    (\"models\", \"MLP\"),\n",
    "    (\"network\", \"MLP\"),\n",
    "    (\"net\", \"MLP\"),\n",
    "    (\"mlp_model\", \"MLP\"),\n",
    "]\n",
    "\n",
    "def try_import_repo_mlp():\n",
    "    \"\"\"\n",
    "    Try to import an MLP implementation from the repository. Returns a callable factory\n",
    "    that accepts (input_dim, hidden_sizes, output_dim, activation) and returns nn.Module,\n",
    "    or None if no suitable repo MLP is found.\n",
    "    \"\"\"\n",
    "    for module_name, class_name in CANDIDATES:\n",
    "        try:\n",
    "            mod = importlib.import_module(module_name)\n",
    "        except Exception:\n",
    "            continue\n",
    "        # try attribute\n",
    "        cls = getattr(mod, class_name, None)\n",
    "        if cls is None:\n",
    "            # maybe module itself is a class\n",
    "            if inspect.isclass(mod) and mod.__name__.lower().startswith(\"mlp\"):\n",
    "                cls = mod\n",
    "        if cls is None:\n",
    "            continue\n",
    "        # Check constructor signature to see if activation param is supported\n",
    "        sig = None\n",
    "        try:\n",
    "            sig = inspect.signature(cls)\n",
    "        except Exception:\n",
    "            pass\n",
    "        def factory(input_dim, hidden_sizes, output_dim, activation):\n",
    "            \"\"\"\n",
    "            Try to instantiate repo MLP. If it supports activation argument, pass it.\n",
    "            Else try positional args (input, hidden, output) otherwise fall back to local wrapper.\n",
    "            \"\"\"\n",
    "            # If class expects an activation argument, pass it\n",
    "            try:\n",
    "                if sig and \"activation\" in sig.parameters:\n",
    "                    return cls(input_dim, hidden_sizes, output_dim, activation=activation)\n",
    "                # try common alternative names\n",
    "                for name in (\"act_fn\", \"nonlinearity\", \"activation_fn\"):\n",
    "                    if sig and name in sig.parameters:\n",
    "                        return cls(input_dim, hidden_sizes, output_dim, **{name: activation})\n",
    "                # try simple constructor patterns\n",
    "                try:\n",
    "                    return cls(input_dim, hidden_sizes, output_dim)\n",
    "                except Exception:\n",
    "                    # try only hidden sizes\n",
    "                    try:\n",
    "                        return cls(hidden_sizes, activation=activation)\n",
    "                    except Exception:\n",
    "                        raise\n",
    "            except Exception:\n",
    "                raise\n",
    "        # test instantiate with dummy args (do not fail hard here)\n",
    "        try:\n",
    "            _ = factory(2, HIDDEN_SIZES, 2, nn.ReLU())\n",
    "            print(f\"Using repository MLP: {module_name}.{class_name}\")\n",
    "            return factory\n",
    "        except Exception:\n",
    "            # not compatible, continue searching\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# Local fallback MLP builder\n",
    "def build_local_mlp(input_dim, hidden_sizes, output_dim, activation):\n",
    "    layers = []\n",
    "    prev = input_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.append(nn.Linear(prev, h))\n",
    "        layers.append(activation if isinstance(activation, nn.Module) else activation())\n",
    "        prev = h\n",
    "    layers.append(nn.Linear(prev, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Build activations map\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": lambda: nn.ReLU(),\n",
    "    \"tanh\": lambda: nn.Tanh(),\n",
    "    \"sigmoid\": lambda: nn.Sigmoid(),\n",
    "    \"leaky_relu\": lambda: nn.LeakyReLU(0.1),\n",
    "    \"elu\": lambda: nn.ELU(),\n",
    "    \"swish\": lambda: nn.SiLU(),  # SiLU is a good swish approximation\n",
    "}\n",
    "\n",
    "# Try to get repo MLP factory\n",
    "repo_factory = try_import_repo_mlp()\n",
    "\n",
    "def make_model(act_name):\n",
    "    act_ctor = ACTIVATIONS[act_name]\n",
    "    activation_module = act_ctor()\n",
    "    if repo_factory is not None:\n",
    "        try:\n",
    "            m = repo_factory(2, HIDDEN_SIZES, 2, activation_module)\n",
    "            if isinstance(m, nn.Module):\n",
    "                return m.to(DEVICE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback\n",
    "    return build_local_mlp(2, HIDDEN_SIZES, 2, activation_module).to(DEVICE)\n",
    "\n",
    "# Prepare dataset\n",
    "X, y = make_moons(n_samples=DATASET_N_SAMPLES, noise=0.2, random_state=SEED)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Training utilities\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def train_one(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "    return history\n",
    "\n",
    "# Run experiments\n",
    "results = {}\n",
    "for act_name in ACTIVATIONS.keys():\n",
    "    print(f\"Training with activation: {act_name}\")\n",
    "    model = make_model(act_name)\n",
    "    hist = train_one(model, train_loader, val_loader)\n",
    "    results[act_name] = {\"model\": model, \"history\": hist}\n",
    "\n",
    "# Plotting: loss and accuracy curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, res in results.items():\n",
    "    plt.plot(res[\"history\"][\"train_loss\"], label=f\"{name} train\", alpha=0.6)\n",
    "    plt.plot(res[\"history\"][\"val_loss\"], linestyle=\"--\", label=f\"{name} val\", alpha=0.9)\n",
    "plt.title(\"Loss curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-entropy\")\n",
    "plt.legend(fontsize=\"small\", ncol=2)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, res in results.items():\n",
    "    plt.plot(res[\"history\"][\"val_acc\"], label=name)\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize=\"small\", ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decision boundary plots\n",
    "xx_min, xx_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "yy_min, yy_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(xx_min, xx_max, 200), np.linspace(yy_min, yy_max, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_scaled = scaler.transform(grid)\n",
    "grid_t = torch.tensor(grid_scaled, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "n = len(results)\n",
    "cols = 3\n",
    "rows = math.ceil(n / cols)\n",
    "plt.figure(figsize=(4 * cols, 4 * rows))\n",
    "i = 1\n",
    "for name, res in results.items():\n",
    "    model = res[\"model\"]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid_t)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "    Z = probs.reshape(xx.shape)\n",
    "    plt.subplot(rows, cols, i)\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap=\"RdYlBu\", alpha=0.8)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor=\"k\", cmap=\"bwr\", s=20, alpha=0.6)\n",
    "    plt.title(f\"Decision boundary: {name}\")\n",
    "    plt.xlim(xx_min, xx_max)\n",
    "    plt.ylim(yy_min, yy_max)\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summarize final validation accuracies\n",
    "print(\"Final validation accuracies:\")\n",
    "for name, res in results.items():\n",
    "    acc = res[\"history\"][\"val_acc\"][-1]\n",
    "    print(f\"  {name}: {acc:.4f}\")\n",
    "\n",
    "# Save figures to files for later review\n",
    "out_dir = Path(\"mlp_activation_experiments\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(out_dir / \"decision_boundaries.png\", bbox_inches=\"tight\")\n",
    "# Note: previous loss/accuracy figure was shown inline; if rerunning, you can save it similarly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
